{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG AI - Natural Language Processing and Speech Recognition\n",
    "# Project: Word Analogies\n",
    "\n",
    "DESCRIPTION\n",
    "\n",
    "In this project, I will show you how to train and evaluate Word2Vec models in  your business data using NLP.<br>\n",
    "Word2Vec is a widely featured as a member of the “new wave” of machine learning algorithms based on neural networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow). Using large amounts of unannotated plain text, word2vec learns relationships between words automatically. The output are vectors, one vector per word, with remarkable linear relationships that allow us to do things like vec(“king”) – vec(“man”) + vec(“woman”) =~ vec(“queen”), or vec(“Montreal Canadiens”) – vec(“Montreal”) + vec(“Toronto”) resembles the vector for “Toronto Maple Leafs”.<br>\n",
    "Word2Vec is very useful in automatic text tagging, recommender systems, and machine translation. Here, I will show you the Python program to generate word vectors using Word2Vec.<br>\n",
    "\n",
    "By Edson Teixeira<br>\n",
    "teixeiraedson252@gmail.com <br>\n",
    "December 29th 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Imports\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "import gensim \n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Datase\n",
    "sample = open(\"word_analogy.txt\", \"r\", encoding='cp1252') \n",
    "s = sample.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Replaces escaped character with space\n",
    "f = s.replace(\"\\n\", \" \") \n",
    "data=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Iterate each sentence in the file\n",
    "for i in sent_tokenize(f): \n",
    "    temp = [] \n",
    "      \n",
    "    # tokenize the sentence into words \n",
    "    for j in word_tokenize(i): \n",
    "        temp.append(j.lower()) \n",
    "  \n",
    "    data.append(temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create CBOW (Continuous Bag of Words) Model\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 1,  \n",
    "                              size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - CBOW :  0.99951744\n",
      "Cosine similarity between 'alice' and 'machines' - CBOW :  0.9920805\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Print results\n",
    "print(\"Cosine similarity between 'alice' \" + \n",
    "               \"and 'wonderland' - CBOW : \", \n",
    "    model1.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" +\n",
    "                 \"and 'machines' - CBOW : \", \n",
    "      model1.similarity('alice', 'machines')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between 'alice' and 'wonderland' - Skip Gram :  0.8932715\n",
      "Cosine similarity between 'alice' and 'machines' - Skip Gram :  0.8536392\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Create a Skip Gram Model and print results\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, size = 100, window = 5, sg = 1) \n",
    "\n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'wonderland' - Skip Gram : \", model2.similarity('alice', 'wonderland')) \n",
    "      \n",
    "print(\"Cosine similarity between 'alice' \" + \"and 'machines' - Skip Gram : \", model2.similarity('alice', 'machines')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
