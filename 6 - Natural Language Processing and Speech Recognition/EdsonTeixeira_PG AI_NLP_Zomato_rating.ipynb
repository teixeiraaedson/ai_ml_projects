{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG AI - Natural Language Processing and Speech Recognition\n",
    "# Project: Build a speech-to-text Model with TensorFlow Dataset\n",
    "\n",
    "#### DESCRIPTION\n",
    "\n",
    "Using NLP and machine learning, make a model to predict the rating in a review based on the content of the text review. This will help identify cases with a mismatch.<br>\n",
    "\n",
    "#### Problem Statement:  \n",
    "\n",
    "Zomato is India’s largest platform for discovering restaurants and ordering food. It operates in India as well as a few cities internationally. Bangalore is one of the biggest customers and restaurant bases for Zomato with 4 to 5 million users using the platform each month.<br>\n",
    "\n",
    "Users on the platform can also post reviews of restaurants and provide a rating accompanying the review. The content in the reviews should ideally reflect the rating provided by the customer. In many cases, there is a mismatch, owing to multiple reasons, where the rating does not match the customer review. The reviews and rating match is very important as it builds customer trust on the platform and helps the user get an accurate picture of the restaurant.<br> \n",
    "\n",
    "You, as a data scientist, need to enable the identification and cleanup of such cases to ensure the ratings reflect the reviews and that the reviews seem trustworthy to the customer. You will need to use NLP techniques in conjunction with machine learning models to predict the rating from the review text. <br>\n",
    "\n",
    "#### Domain: \n",
    "Hospitality and internet\n",
    "\n",
    "Analysis to be done: Perform specific data cleanup, build a rating prediction model using the Random Forest technique and NLP. \n",
    "\n",
    "#### Content: \n",
    "\n",
    "rating:\n",
    "the rating given by the customer\n",
    "\n",
    "review_text: the text in the review\n",
    "\n",
    "#### Steps to perform:\n",
    "\n",
    "Perform clean up on the data; tweak the stop words (negative terms are important). Follow up with a Random Forest Regressor to predict the star rating given by the customers.\n",
    "\n",
    "#### Tasks: \n",
    "\n",
    "1. Load the data using read_csv function from pandas package\n",
    "2. Null values in the review text? \n",
    "    1. Remove the records where the review text is null\n",
    "3. Perform cleanup on the data \n",
    "    1. Normalize the casing\n",
    "    2. Remove extra line breaks from the text\n",
    "    3. Remoove stop words\n",
    "        1. Note: Terms like ‘no’, ‘not’, ‘don’, ‘won’ are important, don’t remove\n",
    "    4. Remove punctuation\n",
    "4. Separation into train and test sets\n",
    "    1. Use train-test method to divide your data into 2 sets: train and test\n",
    "    2. Use a 70-30 split\n",
    "5. Use TF-IDF values for the terms as features to get into a vector space model\n",
    "    1. Import TF-IDF vectorizer from sklearn\n",
    "    2. Instantiate with a maximum of 5000 terms in your vocabulary\n",
    "    3. Fit and apply on the train set\n",
    "    4. Apply on the test set\n",
    "6. Model building: Random Forest Regressor\n",
    "    1. Instantiate RandomForestRegressor from sklearn (set random seed)\n",
    "    2. Fit on the train data\n",
    "    3. Make predictions for the train set\n",
    "7. Model evaluation\n",
    "    1. Report the root mean square error\n",
    "8. Hyperparameter tuning\n",
    "    1. Import GridSearch\n",
    "    2. Provide the parameter grid to choose:\n",
    "        1. max_features – ‘auto’, ‘sqrt’, ‘log2’\n",
    "        2. max_depth – 10, 15, 20, 25\n",
    "9. Find the parameters with the best mean square error in cross-validation\n",
    "    1. Choose the appropriate scoring as the metric for scoring\n",
    "    2. Choose stratified 5 fold cross-validation scheme\n",
    "    3. Fit on the train set\n",
    "10. What are the best parameters?\n",
    "11. Predict and evaluate using the best estimator\n",
    "    1. Use the best estimator from the grid search to make predictions on the test set\n",
    "    2. What is the root mean squared error on the test set?\n",
    "12. Can you identify mismatch cases? \n",
    "    1. Make a rule based on the predicted value and the actual value that identifies mismatch cases (e.g. difference in actual and predicted being more than a cutoff)\n",
    "    2. How many such cases do you see?\n",
    "    3. Are all these mismatch cases genuine?\n",
    "\n",
    "By Edson Teixeira<br>\n",
    "teixeiraedson252@gmail.com <br>\n",
    "December 29th 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews0 = pd.read_csv(\"Zomato_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Their service is worst, pricing in menu is dif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>really appreciate their quality and timing . I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Went there on a Friday night, the place was su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>A very decent place serving good food.\\r\\nOrde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>One of the BEST places for steaks in the city....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                        review_text\n",
       "0     1.0  Their service is worst, pricing in menu is dif...\n",
       "1     5.0  really appreciate their quality and timing . I...\n",
       "2     4.0  Went there on a Friday night, the place was su...\n",
       "3     4.0  A very decent place serving good food.\\r\\nOrde...\n",
       "4     5.0  One of the BEST places for steaks in the city...."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews0.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>27762.000000</td>\n",
       "      <td>27748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>10548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.665784</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.284573</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              rating review_text\n",
       "count   27762.000000       27748\n",
       "unique           NaN       10548\n",
       "top              NaN        good\n",
       "freq             NaN         278\n",
       "mean        3.665784         NaN\n",
       "std         1.284573         NaN\n",
       "min         1.000000         NaN\n",
       "25%         3.000000         NaN\n",
       "50%         4.000000         NaN\n",
       "75%         5.000000         NaN\n",
       "max         5.000000         NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews0.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14 rows are missing the review text - need to get rid of these records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews1 = reviews0[~reviews0.review_text.isnull()].copy()\n",
    "reviews1.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27762, 2), (27748, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews0.shape, reviews1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to list for easy manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_list = reviews1.review_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27748"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text clean up \n",
    "- Normalize the case  \n",
    "- Remove stop words\n",
    "   - remove \"not\", \"no\" from the stop word list\n",
    "- Remove punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lower = [txt.lower() for txt in reviews_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went there on a friday night, the place was surprisingly empty. interesting menu which is almost fully made of dosas. i had bullseye dosa and cheese masala dosa. the bullseye dosa was really good, with the egg perfectly cooked to a half boiled state. the masala in the cheese masala was good, but the cheese was a bit too chewy for my liking. the chutney was good, the sambar was average. the dishes are reasonably priced.',\n",
       " 'a very decent place serving good food.\\r\\nordered chilli fish, chicken & pork sizzler.\\r\\neverything tasted good but pork could have been slightly better cooked.\\r\\ntried 2 beverages, both were very sweet.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_lower[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove extra line breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lower = [\" \".join(txt.split()) for txt in reviews_lower]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['went there on a friday night, the place was surprisingly empty. interesting menu which is almost fully made of dosas. i had bullseye dosa and cheese masala dosa. the bullseye dosa was really good, with the egg perfectly cooked to a half boiled state. the masala in the cheese masala was good, but the cheese was a bit too chewy for my liking. the chutney was good, the sambar was average. the dishes are reasonably priced.',\n",
       " 'a very decent place serving good food. ordered chilli fish, chicken & pork sizzler. everything tasted good but pork could have been slightly better cooked. tried 2 beverages, both were very sweet.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_lower[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Fortnite\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['their', 'service', 'is', 'worst', ',', 'pricing', 'in', 'menu', 'is', 'different', 'from', 'bill', '.', 'they', 'can', 'give', 'you', 'a', 'bill', 'with', 'increased', 'pricing', '.', 'even', 'for', 'serving', 'water', ',', 'menu', ',', 'order', 'you', 'need', 'to', 'call', 'them', '3-4', 'times', 'even', 'on', 'a', 'non', 'busy', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(reviews_lower[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['their', 'service', 'is', 'worst', ',', 'pricing', 'in', 'menu', 'is', 'different', 'from', 'bill', '.', 'they', 'can', 'give', 'you', 'a', 'bill', 'with', 'increased', 'pricing', '.', 'even', 'for', 'serving', 'water', ',', 'menu', ',', 'order', 'you', 'need', 'to', 'call', 'them', '3-4', 'times', 'even', 'on', 'a', 'non', 'busy', 'day', '.']\n"
     ]
    }
   ],
   "source": [
    "reviews_tokens = [word_tokenize(sent) for sent in reviews_lower]\n",
    "print(reviews_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk = stopwords.words(\"english\")\n",
    "stop_punct = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk.remove(\"no\")\n",
    "stop_nltk.remove(\"not\")\n",
    "stop_nltk.remove(\"don\")\n",
    "stop_nltk.remove(\"won\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"no\" in stop_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final = stop_nltk + stop_punct + [\"...\", \"``\",\"''\", \"====\", \"must\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stop(sent):\n",
    "    return [term for term in sent if term not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['really',\n",
       " 'appreciate',\n",
       " 'quality',\n",
       " 'timing',\n",
       " 'tried',\n",
       " 'thattil',\n",
       " 'kutti',\n",
       " 'dosa',\n",
       " \"'ve\",\n",
       " 'addicted',\n",
       " 'dosa',\n",
       " 'really',\n",
       " 'chutney',\n",
       " 'really',\n",
       " 'good',\n",
       " 'money',\n",
       " 'worth',\n",
       " 'much',\n",
       " 'better',\n",
       " 'thattukada',\n",
       " 'try']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_stop(reviews_tokens[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_clean = [del_stop(sent) for sent in reviews_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['service worst pricing menu different bill give bill increased pricing even serving water menu order need call 3-4 times even non busy day',\n",
       " \"really appreciate quality timing tried thattil kutti dosa 've addicted dosa really chutney really good money worth much better thattukada try\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_clean = [\" \".join(sent) for sent in reviews_clean]\n",
    "reviews_clean[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate X and Y and perform train test split, 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27748"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews_clean\n",
    "y = reviews1.rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document term matrix using TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19423, 8325)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bow = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19423, 5000), (8325, 5000))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape, X_test_bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(random_state=42)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learner_rf.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = learner_rf.predict(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23720347586757728"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_train_preds)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase the number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_rf = RandomForestRegressor(random_state=42, n_estimators=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=20, random_state=42)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "learner_rf.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_preds = learner_rf.predict(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2507584827360229"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_train_preds)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"class_weights\" was one of the many hyperparameters to tune for the SVM.  \n",
    "\n",
    "Let's find the best hyper-parameters for the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "?RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the learner with a random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_rf = RandomForestRegressor(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'max_features': [500, \"sqrt\", \"log2\", \"auto\"],\n",
    "    'max_depth': [10, 15, 20, 25]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = learner_rf, param_grid = param_grid, \n",
    "                          cv = 5, n_jobs = -1, verbose = 1, scoring = \"neg_mean_squared_error\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestRegressor(random_state=42), n_jobs=-1,\n",
       "             param_grid={'max_depth': [10, 15, 20, 25],\n",
       "                         'max_features': [500, 'sqrt', 'log2', 'auto']},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.fit(X_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  6.74867921,   1.22219944,   0.58779631,  58.66512151,\n",
       "         15.04919705,   2.26479945,   0.85859833, 102.586408  ,\n",
       "         23.29021144,   3.41018662,   1.13018107, 147.58514905,\n",
       "         32.17802038,   4.70092025,   1.47145724, 172.76807294]),\n",
       " 'std_fit_time': array([1.20657767e-01, 2.07682764e-02, 2.56013859e-03, 9.97178069e-01,\n",
       "        1.39555736e-01, 1.81466166e-01, 1.15846183e-02, 1.79787131e+00,\n",
       "        4.76373187e-01, 1.34993868e-01, 1.04844939e-02, 3.11189525e+00,\n",
       "        5.27870770e-01, 2.19607306e-01, 4.62289179e-02, 2.88618824e+01]),\n",
       " 'mean_score_time': array([0.10120068, 0.08299899, 0.07999873, 0.09360189, 0.10840015,\n",
       "        0.11500087, 0.09919982, 0.10613956, 0.1351759 , 0.10937715,\n",
       "        0.10312624, 0.11807513, 0.14415092, 0.13750024, 0.11562572,\n",
       "        0.1093698 ]),\n",
       " 'std_score_time': array([1.36445616e-02, 1.26553768e-03, 4.33455980e-03, 7.96430287e-03,\n",
       "        3.87934817e-03, 1.96774720e-02, 8.23036380e-03, 6.49581865e-03,\n",
       "        1.84154509e-02, 2.80484031e-06, 7.65644703e-03, 5.00678354e-03,\n",
       "        1.16120085e-02, 2.29613385e-02, 7.65504498e-03, 2.61490747e-02]),\n",
       " 'param_max_depth': masked_array(data=[10, 10, 10, 10, 15, 15, 15, 15, 20, 20, 20, 20, 25, 25,\n",
       "                    25, 25],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[500, 'sqrt', 'log2', 'auto', 500, 'sqrt', 'log2',\n",
       "                    'auto', 500, 'sqrt', 'log2', 'auto', 500, 'sqrt',\n",
       "                    'log2', 'auto'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 10, 'max_features': 500},\n",
       "  {'max_depth': 10, 'max_features': 'sqrt'},\n",
       "  {'max_depth': 10, 'max_features': 'log2'},\n",
       "  {'max_depth': 10, 'max_features': 'auto'},\n",
       "  {'max_depth': 15, 'max_features': 500},\n",
       "  {'max_depth': 15, 'max_features': 'sqrt'},\n",
       "  {'max_depth': 15, 'max_features': 'log2'},\n",
       "  {'max_depth': 15, 'max_features': 'auto'},\n",
       "  {'max_depth': 20, 'max_features': 500},\n",
       "  {'max_depth': 20, 'max_features': 'sqrt'},\n",
       "  {'max_depth': 20, 'max_features': 'log2'},\n",
       "  {'max_depth': 20, 'max_features': 'auto'},\n",
       "  {'max_depth': 25, 'max_features': 500},\n",
       "  {'max_depth': 25, 'max_features': 'sqrt'},\n",
       "  {'max_depth': 25, 'max_features': 'log2'},\n",
       "  {'max_depth': 25, 'max_features': 'auto'}],\n",
       " 'split0_test_score': array([-0.84805792, -1.21873038, -1.46601418, -0.8797774 , -0.68067359,\n",
       "        -1.06262567, -1.38275701, -0.71735493, -0.56964763, -0.92435109,\n",
       "        -1.30573765, -0.60233922, -0.48913115, -0.81727014, -1.24249308,\n",
       "        -0.52429385]),\n",
       " 'split1_test_score': array([-0.83771019, -1.21582872, -1.47551313, -0.80785244, -0.65444803,\n",
       "        -1.05580533, -1.38368675, -0.64953241, -0.53849386, -0.9161609 ,\n",
       "        -1.30492522, -0.55219975, -0.45713093, -0.79742469, -1.23217209,\n",
       "        -0.47236955]),\n",
       " 'split2_test_score': array([-0.82450929, -1.22483994, -1.48621782, -0.81362485, -0.64687216,\n",
       "        -1.05401849, -1.39503427, -0.65762943, -0.53220825, -0.92064256,\n",
       "        -1.31206218, -0.56072208, -0.4502666 , -0.79870079, -1.2403262 ,\n",
       "        -0.48929824]),\n",
       " 'split3_test_score': array([-0.77249261, -1.15251984, -1.39456468, -0.78612728, -0.61330314,\n",
       "        -0.99807807, -1.30220425, -0.64669395, -0.51013155, -0.86225407,\n",
       "        -1.22585946, -0.55193671, -0.43676118, -0.75441706, -1.15460954,\n",
       "        -0.48655494]),\n",
       " 'split4_test_score': array([-0.80473447, -1.20175172, -1.46919791, -0.79873802, -0.63530674,\n",
       "        -1.03550837, -1.3728384 , -0.65650155, -0.52472224, -0.90267622,\n",
       "        -1.2916304 , -0.5567857 , -0.44895519, -0.78340279, -1.22382812,\n",
       "        -0.48781044]),\n",
       " 'mean_test_score': array([-0.8175009 , -1.20273412, -1.45830154, -0.817224  , -0.64612073,\n",
       "        -1.04120719, -1.36730413, -0.66554246, -0.53504071, -0.90521697,\n",
       "        -1.28804298, -0.56479669, -0.45644901, -0.79024309, -1.21868581,\n",
       "        -0.49206541]),\n",
       " 'std_test_score': array([0.02675815, 0.02622386, 0.03260724, 0.03262651, 0.02217091,\n",
       "        0.02336058, 0.03330095, 0.02623156, 0.01972067, 0.02269845,\n",
       "        0.03179526, 0.01904879, 0.01761009, 0.02089911, 0.03270836,\n",
       "        0.01722049]),\n",
       " 'rank_test_score': array([ 9, 12, 16,  8,  5, 11, 15,  6,  3, 10, 14,  4,  1,  7, 13,  2])}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=25, max_features=500, random_state=42)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the best estimator to make predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = grid_search.best_estimator_.predict(X_train_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = grid_search.best_estimator_.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5876681060137491"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_train, y_train_pred)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6715338514796404"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_test_pred)**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying mismatch cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame({'review':X_test, 'rating':y_test, 'rating_pred':y_test_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[(res_df.rating - res_df.rating_pred)>=2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>life saviours serving excellent food worst tim...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.007670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>value money ordered second time</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.970624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>not good</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.998553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16510</th>\n",
       "      <td>may not polished serving packaging etc never b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.894671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14845</th>\n",
       "      <td>oh memories place first drink bangalore almost...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.628792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15201</th>\n",
       "      <td>sauce not included</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.878766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>rice quantity less</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.844196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16515</th>\n",
       "      <td>may not polished serving packaging etc never b...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.894671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  rating  rating_pred\n",
       "7277   life saviours serving excellent food worst tim...     5.0     2.007670\n",
       "1818                     value money ordered second time     5.0     2.970624\n",
       "4771                                            not good     5.0     1.998553\n",
       "16510  may not polished serving packaging etc never b...     5.0     1.894671\n",
       "14845  oh memories place first drink bangalore almost...     5.0     2.628792\n",
       "15201                                 sauce not included     4.0     1.878766\n",
       "3165                                  rice quantity less     5.0     2.844196\n",
       "16515  may not polished serving packaging etc never b...     5.0     1.894671"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df[(res_df.rating - res_df.rating_pred)>=2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
