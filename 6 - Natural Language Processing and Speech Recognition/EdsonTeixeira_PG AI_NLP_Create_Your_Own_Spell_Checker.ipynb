{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cfs22.simplicdn.net/ice9/new_logo.svgz \"/>\n",
    "\n",
    "# PG AI - Natural Language Processing and Speech Recognition\n",
    "# Assisted Practice: Create Your Own Spell Checker\n",
    "\n",
    "#### DESCRIPTION\n",
    "\n",
    "Creating a spell checker, correct the incorrect word in the given sentence.<br>\n",
    "\n",
    "#### Problem Statement: \n",
    "While typing or sending any message to person, we generally make spelling mistakes. Write a script which will correct the misspelled words in a sentence. The input will be a raw string and the output will be a string with the case normalized and the incorrect word corrected.<br>\n",
    "\n",
    "#### Domain: General\n",
    "\n",
    "#### Analysis to be done: Words availability in corpus\n",
    "\n",
    "#### Content:\n",
    "Dataset: None\n",
    "\n",
    "We will be using NLTK’s inbuilt corpora (words, stop words etc.) and no specific dataset.\n",
    "\n",
    "#### Steps to perform:\n",
    "\n",
    "While there are several approaches to correct spelling , you will use the  Levenshtein or Edit distance approach. \n",
    "\n",
    "The approach will be straightforward for correcting a word:  \n",
    "\n",
    "    If the word is present in a list of valid words, the word is correct.\n",
    "\n",
    "    If the word is absent from the valid word list, we will find the correct word, i.e., the word from the valid word list which has the lowest edit distance from the target word.\n",
    "\n",
    "Once you define a function, you will iterate over the terms in the given sentence, correct the words identified as incorrect, and return a joined string with all the terms. To help speed up execution, you won’t be applying the spell check on the stop words and punctuation.\n",
    "\n",
    "#### Tasks: \n",
    "\n",
    "1. Get a list of valid words in the English language using NLTK’s list of words (Hint: use nltk.download(‘words’) to get the raw list.\n",
    "2. Look at the first 20 words in the list. Is the casing normalized?\n",
    "3. Normalize the casing for all the terms.\n",
    "4. Some duplicates would have been induced, create unique list after normalizing.\n",
    "5. Create a list of stop words which should include: \n",
    "\n",
    "    1. Stop words from NLTK\n",
    "    2. All punctuations (Hint: use ‘punctuation’ from string module)\n",
    "    3. Final list should be a combination of these two\n",
    "\n",
    "6. Define a function to get correct a single term\n",
    "\n",
    "    1. For a given term, find its edit distance with each term in the valid word list. To speed up execution, you can use the first 20,000 entries in the valid word list.\n",
    "    2. Store the result in a dictionary, the key as the term, and edit distance as value.\n",
    "    3. Sort the dictionary in ascending order of the values.\n",
    "    4. Return the first entry in the sorted result (value with minimum edit distance).\n",
    "    5. Using the function, get the correct word for committee.\n",
    "\n",
    "7. Make a set from the list of valid words, for faster lookup to see if word is in valid list or not.\n",
    "8. Define a function for spelling correction in any given input sentence:\n",
    "    1. Tokenize them after making all the terms in lowercase \n",
    "    2. For each term in the tokenized sentence:\n",
    "        1. Check if the term is in the list of valid words (valid_words_set).\n",
    "        2. If yes, return the word as is.\n",
    "        3. If no, get the correct word using get_correct_term function.\n",
    "\n",
    "    3. To return the joined string as output.\n",
    "\n",
    "9. Test the function for the input sentence “The new abacos is great”.\n",
    "\n",
    "By Edson Teixeira<br>\n",
    "teixeiraedson252@gmail.com <br>\n",
    "December 30th 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get a list of valid words in the English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/labsuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = nltk.corpus.words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We see that case has not been normalized. Normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'Aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'Aaron',\n",
       " 'Aaronic',\n",
       " 'Aaronical',\n",
       " 'Aaronite',\n",
       " 'Aaronitic',\n",
       " 'Aaru',\n",
       " 'Ab',\n",
       " 'aba',\n",
       " 'Ababdeh',\n",
       " 'Ababua',\n",
       " 'abac']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_words[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get unique list of words after normalizing case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = [term.lower() for term in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aaronic',\n",
       " 'aaronical',\n",
       " 'aaronite',\n",
       " 'aaronitic',\n",
       " 'aaru',\n",
       " 'ab',\n",
       " 'aba',\n",
       " 'ababdeh',\n",
       " 'ababua',\n",
       " 'abac']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words = list(sorted(set(valid_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234377"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aani',\n",
       " 'aardvark',\n",
       " 'aardwolf',\n",
       " 'aaron',\n",
       " 'aaronic']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We needn't apply the spell checker on stop words or punctuations. This will make the code much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final stop word list to be used will be English stopwords from NLTK together with punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_nltk = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_punct = list(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final = stop_nltk + stop_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get correction for a single term\n",
    "1. For a given term, find its edit distance with each term in the valid word list\n",
    "2. Store the result in a dictionary with the key as the term, and the edit distance as the value\n",
    "3. Sort the dictionary in asccending order of the values\n",
    "4. Return the first entry in the sorted result (value with minimum edit distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.edit_distance('salsa', 'salso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correct_term(inp_word):\n",
    "    res_dict = {valid_term:nltk.edit_distance(inp_word, valid_term) for valid_term in valid_words[:20000]}\n",
    "    res_dict_sorted = sorted(res_dict.items(), key=lambda kv: kv[1], reverse=False)\n",
    "    return res_dict_sorted[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abacus'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_term(\"abacus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'admittee'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_correct_term(\"comittee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write the commands to tokenize after lowering case for any given input sentence\n",
    "- Use NLTKs word_tokenize for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_sent = \"The new abacos is great\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = word_tokenize(inp_sent.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'new', 'abacos', 'is', 'great']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a set from valid_words, for faster lookup to check if word is in valid list or not.  \n",
    "Only those words which are not in the valid list need to be corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words_set = set(valid_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a function for spell correcting any given input sentence\n",
    "1. tokenize after doing lower case \n",
    "2. For each term in the tokenized sentence +\n",
    "    - check if the term is in the list of valid words (valid_words_set)\n",
    "    - if yes, return the word as is\n",
    "    - if no, get the correct word using get_correct_term function\n",
    "3. Return the joined string as output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new abacos is great'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_set(inp_sent):\n",
    "    inp_tokens = word_tokenize(inp_sent.lower())\n",
    "    corrected_tokens = [term if ((term in valid_words_set) or (term in stop_final)) else get_correct_term(term) for term in inp_tokens]\n",
    "    return \" \".join(corrected_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the new abacus is great'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_set('The new abacos is great')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
